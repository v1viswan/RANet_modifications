{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from math import log10\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.cuda as cuda\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from RANet_lib import *\n",
    "from RANet_lib.RANet_lib import *\n",
    "from RANet_model import RANet as Net\n",
    "import os\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Setting ......\n",
      "../models/exists\n",
      "using device ID: [0]\n",
      "===> Building model\n",
      "Single-object mode\n",
      "Change to multi-object mode\n",
      "Number of files: 60\n",
      "Dirs: 60\n"
     ]
    }
   ],
   "source": [
    "# params='RANet_video_multi.pth'\n",
    "params='RANet_multi_retrained1_epoch1.pth'\n",
    "dataset='17val'\n",
    "save_root='../predictions/RANet_Video_17val'\n",
    "\n",
    "net_name = 'RANet'\n",
    "parser = argparse.ArgumentParser(description='RANet')\n",
    "parser.add_argument('--deviceID', default=[0], help='device IDs')\n",
    "parser.add_argument('--threads', type=int, default=16, help='number of threads for data loader to use')\n",
    "parser.add_argument('--workfolder', default='../models/')\n",
    "parser.add_argument('--savePName', default=net_name)\n",
    "parser.add_argument('--net_type', default='single_object')\n",
    "parser.add_argument('--fp16', default=True)\n",
    "print('===> Setting ......')\n",
    "# opt = parser.parse_args()\n",
    "class args:\n",
    "    pass\n",
    "opt = args()\n",
    "opt.deviceID = [0]\n",
    "opt.threads=0\n",
    "opt.workfolder='../models/'\n",
    "opt.savePName = net_name\n",
    "opt.net_type='single_object'\n",
    "opt.fp16=False\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "try:\n",
    "    os.mkdir(opt.workfolder)\n",
    "    print('build working folder: ' + opt.workfolder)\n",
    "except:\n",
    "    print(opt.workfolder + 'exists')\n",
    "print('using device ID: {}'.format(opt.deviceID))\n",
    "\n",
    "\n",
    "print('===> Building model')\n",
    "params='RANet_video_multi.pth'\n",
    "dataset='17train'\n",
    "save_root='../predictions/RANet_Video_17val'\n",
    "\n",
    "model = Net(pretrained=False, type=opt.net_type)\n",
    "model_cuda = None\n",
    "inSize1 = 480\n",
    "inSize2 = 864\n",
    "if dataset in ['16val', '16trainval', '16all']:\n",
    "    model.set_type('single_object')\n",
    "    year = '2016'\n",
    "elif dataset in ['17val', '17test_dev', '17train']:\n",
    "    model.set_type('multi_object')\n",
    "    year = '2017'\n",
    "else:\n",
    "    assert('dataset error')\n",
    "\n",
    "DAVIS = dict(reading_type='SVOS',\n",
    "                 year=year,\n",
    "             root='../datasets/DAVIS/',\n",
    "             subfolder=['', '', ''],\n",
    "             mode=dataset,\n",
    "             tar_mode='rep',\n",
    "             train=0, val=0, test=0, predict=1,\n",
    "             length=None,\n",
    "             init_folder=None,\n",
    "             )\n",
    "dataset = DAVIS2017_loader(\n",
    "    [DAVIS], mode='test',\n",
    "    transform=[PAD_transform([inSize1, inSize2], random=False),\n",
    "               PAD_transform([inSize1, inSize2], random=False)],\n",
    "    rand=Rand_num())\n",
    "checkpoint_load(opt.workfolder + params, model)\n",
    "\n",
    "if opt.deviceID==[0]:\n",
    "    model_cuda = model.cuda()\n",
    "else:\n",
    "    model_cuda = nn.DataParallel(model).cuda()\n",
    "if opt.fp16:\n",
    "    model_cuda = model_cuda.half()\n",
    "    model_cuda.fp16 = True\n",
    "\n",
    "# fitpredict17(dataset, model_cuda, add_name=add_name, threads=1, batchSize=1, save_root=save_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Arguments #####\n",
    "data_set = dataset\n",
    "model = model_cuda\n",
    "add_name,threads,batchSize,save_root = '', 1, 1, save_root\n",
    "##### Arguments #####\n",
    "\n",
    "if data_set.Datasets_params[0]['mode'] in ['16val', '16all']:\n",
    "    threshold = 0.5\n",
    "    single_object = True\n",
    "elif data_set.Datasets_params[0]['mode'] in ['17val', '17test_dev']:\n",
    "    threshold = 0.5\n",
    "    single_object = False\n",
    "else:\n",
    "    threshold = 0.5\n",
    "    single_object = False\n",
    "threads=0\n",
    "print('Start testing ..., workers for dataloader:',threads,'batch size:', batchSize)\n",
    "data_set.iter_mode = 'train'\n",
    "data_loader = DataLoader(dataset=data_set, num_workers=threads, batch_size=batchSize, shuffle=False, pin_memory=True)\n",
    "model.eval()\n",
    "if not (os.path.exists(save_root)):\n",
    "    os.mkdir(save_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data .........\n"
     ]
    }
   ],
   "source": [
    "data_loader = data_loader\n",
    "model=model\n",
    "save_root=save_root\n",
    "threshold=threshold\n",
    "single_object=single_object\n",
    "pre_first_frame=False\n",
    "add_name=''\n",
    "\n",
    "\n",
    "ms = [864, 480]\n",
    "palette_path = '../datasets/palette.txt'\n",
    "with open(palette_path) as f:\n",
    "    palette = f.readlines()\n",
    "palette = list(np.asarray([[int(p) for p in pal[0:-1].split(' ')] for pal in palette]).reshape(768))\n",
    "def init_Frame(batchsize):\n",
    "    Key_features = [[] for i in range(batchsize)]\n",
    "    Masks = [[] for i in range(batchsize)]\n",
    "    Init_Key_masks = [[] for i in range(batchsize)]\n",
    "    Frames = [[] for i in range(batchsize)]\n",
    "    Box = [[] for i in range(batchsize)]\n",
    "    Image_names = [[] for i in range(batchsize)]\n",
    "    Img_sizes = [[] for i in range(batchsize)]\n",
    "    Frames_batch = dict(Frames=Frames, Key_features=Key_features, Masks=Masks, Box=Box, Img_sizes=Img_sizes, Init_Key_masks=Init_Key_masks,\n",
    "                        Image_names=Image_names, Sizes=[0 for i in range(batchsize + 1)], batchsize=batchsize, Flags=[[] for i in range(batchsize)],\n",
    "                        Img_flags=[[] for i in range(batchsize)])\n",
    "    return Frames_batch\n",
    "batchsize = 1\n",
    "max_iter = batchsize\n",
    "torch.set_grad_enabled(False)\n",
    "_ = None\n",
    "Frames_batch = init_Frame(batchsize)\n",
    "print('Loading Data .........')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_BCELoss(prediction, target):\n",
    "    eps = 0.000001\n",
    "    ratios = torch.clamp( ((1-target).sum(dim=1)/target.sum(dim=1)).reshape(-1,1), 1/100, 100)\n",
    "    loss = -( ratios*target*torch.log(prediction + eps) +\\\n",
    "         (1-target)*torch.log(1-prediction + eps)).mean()\n",
    "    return loss\n",
    "def custom_BCELoss_with_logits(prediction, target):\n",
    "    eps = 0\n",
    "    ratios = torch.clamp( ((1-target).sum(dim=1)/target.sum(dim=1)).reshape(-1,1), 1/100, 100)\n",
    "    loss = -( ratios*target*F.logsigmoid(prediction + eps) +\\\n",
    "         (1-target)*F.logsigmoid(-prediction + eps)).mean()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)\n",
    "loss_per_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got first batch of images for training, iteration: 8 time diff now: 278.7771303448826\n",
      "Gonna start training model\n",
      "max targets: tensor(3., device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.UpsamplingBilinear2d is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "max targets: tensor(3., device='cuda:0')\n",
      "Time to train model: 307.1244905409403 Total Time for one batch video: 585.9017987567931 memory used 1052.49853515625\n",
      "Checkpoint saved to ../models/RANet_multi_retrained10_epoch1.pth\n",
      "Got first batch of images for training, iteration: 10 time diff now: 640.7487410688773\n",
      "Gonna start training model\n",
      "max targets: tensor(4., device='cuda:0')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 10.12 MiB (GPU 0; 10.92 GiB total capacity; 9.34 GiB already allocated; 5.50 MiB free; 115.11 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-a7bf1b703ae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 Out, corr_loss_batch = model.RANet_Multiple_forward_train(template=template,target=target,\\\n\u001b[0;32m---> 69\u001b[0;31m                                                 template_msk=template_mask, target_msk = target_mask)\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mprediction_single_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mtarget_single_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vj_data/RANet/codes/RANet_model.py\u001b[0m in \u001b[0;36mRANet_Multiple_forward_train\u001b[0;34m(self, template, target, template_msk, target_msk)\u001b[0m\n\u001b[1;32m    598\u001b[0m                     \u001b[0mFG_BG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrelate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mm2_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mm2_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mb_iter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                     \u001b[0mBG_FG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrelate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm2_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mm2_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mb_iter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                     \u001b[0mBG_BG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrelate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mm2_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mm2_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mb_iter\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m                 \u001b[0mFG_FG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFG_FG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                 \u001b[0mFG_BG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFG_BG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/vj_data/RANet/codes/RANet_model.py\u001b[0m in \u001b[0;36mcorrelate\u001b[0;34m(self, Kernel, Feature)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcorrelate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKernel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mcorr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKernel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcorr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 10.12 MiB (GPU 0; 10.92 GiB total capacity; 9.34 GiB already allocated; 5.50 MiB free; 115.11 MiB cached)"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1):\n",
    "    start_time = time.perf_counter()\n",
    "    loss_per_batch = []\n",
    "    for iteration, batch in enumerate(data_loader, 1):\n",
    "        if (iteration < 7):\n",
    "            continue\n",
    "        if model.fp16:\n",
    "            batch[0] = [datas.half() for datas in batch[0]]\n",
    "            batch[1] = [datas.half() for datas in batch[1]]\n",
    "        else:\n",
    "            batch[0] = [datas for datas in batch[0]]\n",
    "            batch[1] = [datas for datas in batch[1]]\n",
    "        frame_num = len(batch[0])\n",
    "        size = batch[0][0].size()[2::]\n",
    "        # cc for key frame\n",
    "        Frames = batch[0]\n",
    "        Img_sizes = batch[3]\n",
    "\n",
    "        loc = np.argmin(Frames_batch['Sizes'][0:batchsize])\n",
    "        Fsize = len(batch[2])\n",
    "        # print(loc)\n",
    "        # print(Fsize)\n",
    "        Frames_batch['Frames'][loc].extend(batch[0])\n",
    "        Frames_batch['Masks'][loc].extend(batch[1])\n",
    "        Frames_batch['Sizes'][loc] += Fsize \n",
    "\n",
    "        if iteration % max_iter == 0 or iteration == len(data_loader):\n",
    "            print(\"Got first batch of images for training, iteration:\", iteration,\\\n",
    "                  \"time diff now:\", time.perf_counter()-start_time)\n",
    "        else : \n",
    "            continue\n",
    "        ########### Once we have a whole minibatch of videos, train the network\n",
    "        print(\"Gonna start training model\")\n",
    "        start_time_model = time.perf_counter()\n",
    "        loss_per_mini_batch = []\n",
    "        for idx in range(max(Frames_batch['Sizes'])):\n",
    "            optimizer.zero_grad()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            template = [[] for i in range(batchsize)]\n",
    "            target = [[] for i in range(batchsize)]\n",
    "            template_mask = [[] for i in range(batchsize)]\n",
    "            target_mask = [[] for i in range(batchsize)]\n",
    "            \n",
    "            for batch_id, (frame2, mask2, frame1, mask1) in enumerate(\\\n",
    "                    zip([i[idx%(len(i)-1)] for i in Frames_batch['Frames']],\\\n",
    "                        [i[idx%(len(i)-1)] for i in Frames_batch['Masks']],\\\n",
    "                        [i[idx%(len(i)-1) +1] for i in Frames_batch['Frames']],\\\n",
    "                        [i[idx%(len(i)-1) +1] for i in Frames_batch['Masks']])):\n",
    "                template[batch_id] = frame2\n",
    "                template_mask[batch_id] = mask2\n",
    "                target[batch_id] = frame1\n",
    "                target_mask[batch_id] = mask1\n",
    "\n",
    "            template = torch.cat(template,dim=0)\n",
    "            template_mask = torch.cat(template_mask)\n",
    "            target = torch.cat(target)\n",
    "            target_mask = torch.cat(target_mask)\n",
    "            \n",
    "            template = template.cuda()\n",
    "            template_mask = template_mask.cuda()\n",
    "            target = target.cuda()\n",
    "            target_mask = target_mask.cuda()\n",
    "            print(\"max targets:\", target_mask.max())\n",
    "        #     with torch.no_grad():\n",
    "            with torch.enable_grad():\n",
    "                Out, corr_loss_batch = model.RANet_Multiple_forward_train(template=template,target=target,\\\n",
    "                                                template_msk=template_mask, target_msk = target_mask)\n",
    "                prediction_single_masks = []\n",
    "                target_single_masks = []\n",
    "                for idx in range(len(Out)):\n",
    "                    max_obj = template_mask[idx,0].max().int().data.cpu().numpy()\n",
    "                    target_msk_images = model.P2masks(F.relu(target_mask[idx,0] - 1), max_obj - 1)\n",
    "                    for i in range(max_obj-1):\n",
    "                        prediction_single_masks.append(Out[idx][0,i].reshape(-1))\n",
    "                        target_single_masks.append(target_msk_images[i+1].reshape(-1))\n",
    "\n",
    "\n",
    "                prediction_single_masks = torch.stack(prediction_single_masks)\n",
    "                target_single_masks = torch.stack(target_single_masks)\n",
    "\n",
    "                cls_loss = custom_BCELoss_with_logits(prediction_single_masks,target_single_masks)\n",
    "                lamda1= 0.5\n",
    "                lamda2 = 0.5\n",
    "                lamda = 0.001\n",
    "                correlation_loss = 0\n",
    "                count = 0\n",
    "                for idx in range(len(corr_loss_batch)):  \n",
    "                    template_FB_FB_loss = 0\n",
    "                    template_FB_BG_loss = 0\n",
    "                    target_FB_FB_loss = 0\n",
    "                    target_FB_BG_loss = 0\n",
    "                    tt_FB_FB_loss = 0\n",
    "                    tt_FB_BG_loss = 0\n",
    "                    for idy in range(len(corr_loss_batch[idx])):\n",
    "                        template_FB_FB_loss += corr_loss_batch[idx][idy]['template_FB_FB_loss']\n",
    "                        template_FB_BG_loss += corr_loss_batch[idx][idy]['template_FB_BG_loss']\n",
    "                        target_FB_FB_loss += corr_loss_batch[idx][idy]['target_FB_FB_loss']\n",
    "                        target_FB_BG_loss += corr_loss_batch[idx][idy]['target_FB_BG_loss']\n",
    "                        tt_FB_FB_loss += corr_loss_batch[idx][idy]['tt_FB_FB_loss']\n",
    "                        tt_FB_BG_loss += corr_loss_batch[idx][idy]['tt_FB_BG_loss']\n",
    "                        count += 1\n",
    "\n",
    "                    correlation_loss += lamda1*(template_FB_FB_loss+target_FB_FB_loss+tt_FB_FB_loss)/3 +\\\n",
    "                                    lamda2*(template_FB_BG_loss+target_FB_BG_loss+tt_FB_BG_loss)/3\n",
    "                correlation_loss  = correlation_loss/count \n",
    "                total_loss = cls_loss + lamda*correlation_loss\n",
    "            \n",
    "            if np.isnan(total_loss.item()):\n",
    "                print(\"Nan value for loss!, breaking\")\n",
    "                asdsad\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_per_mini_batch.append([total_loss.item(), cls_loss.item(),correlation_loss.item() ])\n",
    "            del total_loss, correlation_loss, cls_loss, Out, corr_loss_batch,\\\n",
    "                    template, target, template_mask, target_mask\n",
    "        ######### Reinitialize Frames for the next set of videos\n",
    "        Frames_batch = init_Frame(batchsize)\n",
    "        loss_per_mini_batch = np.array(loss_per_mini_batch)\n",
    "        loss_per_batch.append(loss_per_mini_batch.mean())\n",
    "        end_time_model = time.perf_counter()\n",
    "        memory = cuda.memory_allocated(0) /(1024*1024)\n",
    "        print(\"Time to train model:\", end_time_model - start_time_model,\\\n",
    "              \"Total Time for one batch video:\", end_time_model-start_time, \"memory used\",memory)\n",
    "        checkpoint_save(opt.workfolder + 'RANet_multi_retrained1', 1, model)\n",
    "        asds\n",
    "    loss_per_epoch.append(np.mean(loss_per_batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f01d6490f60>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(target_mask[0,0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 480, 864])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['../datasets/DAVIS/JPEGImages/480p/classic-car/00000.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00001.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00002.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00003.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00004.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00005.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00006.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00007.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00008.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00009.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00010.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00011.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00012.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00013.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00014.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00015.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00016.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00017.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00018.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00019.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00020.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00021.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00022.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00023.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00024.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00025.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00026.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00027.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00028.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00029.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00030.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00031.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00032.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00033.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00034.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00035.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00036.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00037.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00038.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00039.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00040.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00041.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00042.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00043.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00044.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00045.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00046.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00047.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00048.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00049.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00050.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00051.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00052.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00053.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00054.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00055.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00056.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00057.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00058.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00059.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00060.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00061.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/classic-car/00062.jpg'],\n",
       " ['../datasets/DAVIS/JPEGImages/480p/color-run/00000.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00001.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00002.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00003.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00004.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00005.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00006.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00007.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00008.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00009.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00010.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00011.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00012.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00013.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00014.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00015.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00016.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00017.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00018.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00019.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00020.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00021.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00022.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00023.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00024.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00025.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00026.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00027.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00028.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00029.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00030.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00031.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00032.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00033.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00034.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00035.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00036.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00037.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00038.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00039.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00040.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00041.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00042.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00043.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00044.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00045.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00046.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00047.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00048.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00049.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00050.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00051.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00052.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00053.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00054.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00055.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00056.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00057.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00058.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00059.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00060.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00061.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00062.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00063.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00064.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00065.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00066.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00067.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00068.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00069.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00070.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00071.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00072.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00073.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00074.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00075.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00076.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00077.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00078.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00079.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00080.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00081.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00082.jpg',\n",
       "  '../datasets/DAVIS/JPEGImages/480p/color-run/00083.jpg']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Frames_batch['Image_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4., device='cuda:0')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_mask.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9565.51806640625"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "cuda.memory_allocated(0) /(1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as f\n",
    "self=model\n",
    "template=template\n",
    "target=target\n",
    "template_msk=template_mask\n",
    "target_msk = target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_features2 = self.res_forward(template)\n",
    "Kernel_3 = f.normalize(f.max_pool2d(self.L3(base_features2[2]), 2))\n",
    "Kernel_4 = f.normalize(self.L4(base_features2[3]))\n",
    "Kernel_5 = f.normalize(f.interpolate(self.L5(base_features2[4]), scale_factor=2, mode='bilinear'))\n",
    "Kernel_tmp = f.normalize(self.L_g(torch.cat([Kernel_3, Kernel_4, Kernel_5], dim=1)))\n",
    "Kernel = Kernel_tmp\n",
    "\n",
    "# Current frame feature\n",
    "base_features1 = self.res_forward(target)\n",
    "Feature_3 = f.normalize(f.max_pool2d(self.L3(base_features1[2]), 2))\n",
    "Feature_4 = f.normalize(self.L4(base_features1[3]))\n",
    "Feature_5 = f.normalize(f.interpolate(self.L5(base_features1[4]), scale_factor=2, mode='bilinear'))\n",
    "Feature = f.normalize(self.L_g(torch.cat([Feature_3, Feature_4, Feature_5], dim=1)))\n",
    "\n",
    "Out_Rs = []\n",
    "\n",
    "basef1 = torch.cat([self.ls13(base_features1[2]),\n",
    "                    self.ls14(base_features1[3]),\n",
    "                    self.ls15(base_features1[4]), ], 1)\n",
    "basef2 = torch.cat([self.ls22(base_features1[1]),\n",
    "                    self.ls23(base_features1[2]),\n",
    "                    self.ls24(base_features1[3]), ], 1)\n",
    "basef3 = torch.cat([self.ls31(base_features1[0]),\n",
    "                    self.ls32(base_features1[1]),\n",
    "                    self.ls33(base_features1[2])], 1)\n",
    "loss_per_batch = []\n",
    "for idx in range(len(Feature)):  # batch\n",
    "    ker = Kernel_tmp[idx: idx + 1]\n",
    "    feature = Feature[idx: idx + 1]\n",
    "    m2 = template_msk[idx: idx + 1]\n",
    "    m1 = target_msk[idx: idx + 1]\n",
    "\n",
    "    max_obj = m2.max().int().data.cpu().numpy()\n",
    "    if max_obj < 2:\n",
    "        m2[0, 0, 0, 0] = 2\n",
    "        max_obj = m2.max().int().data.cpu().numpy()\n",
    "    M2s = self.P2masks(f.relu(m2 - 1), max_obj - 1)\n",
    "    M2_all = m2.ge(1.5).float()\n",
    "    M1s = self.P2masks(f.relu(m1 - 1), max_obj - 1)\n",
    "    M1_all = m1.ge(1.5).float()\n",
    "\n",
    "    # Correlation\n",
    "    W0, H0 = ker.size()[-2::]\n",
    "    W,H = feature.size()[-2::]\n",
    "    Corr_subs = []\n",
    "    ker_R = self.to_kernel(ker)\n",
    "    corr_R = self.correlate(ker_R, feature)\n",
    "\n",
    "    template_self_corr = self.correlate(ker_R, ker)\n",
    "    target_self_corr = self.correlate(self.to_kernel(feature), feature)\n",
    "\n",
    "    # Ranking attention scores\n",
    "    T_corr =  f.max_pool2d(corr_R,2).view(-1, W0*H0, W*H//4).transpose(1, 2).view(-1, W*H//4, W0, H0)\n",
    "    R_map = f.relu(self.Ranking(T_corr)) * 0.2\n",
    "    Rmaps = []\n",
    "\n",
    "\n",
    "    for idy in range(max_obj):  # make corrs (backgrounds(=1) and objs)\n",
    "        m2_rep = f.adaptive_avg_pool2d(M2s[idy], ker.size()[-2::])\n",
    "        corr_sub = m2_rep.view(m2_rep.size()[0], -1, 1, 1) * corr_R\n",
    "        Corr_subs.append(corr_sub)\n",
    "        Rmaps.append((R_map * m2_rep).view(-1, 1, W0*H0))\n",
    "\n",
    "    Outs = []\n",
    "    loss_per_obj = []\n",
    "    for idy in range(1, max_obj):  # training:with_bg, testing: w/o BG\n",
    "        corr = Corr_subs[idy]\n",
    "        co_size = Corr_subs[idy].size()[2::]\n",
    "        max_only, indices = f.max_pool2d(corr, co_size, return_indices=True)\n",
    "        max_only = max_only.view(-1, 1, W0*H0) + Rmaps[idy]\n",
    "\n",
    "        #### For FG, adjust scores based on how close a pixel is to rest of the FG pixes and far away from BG pixels\n",
    "        # Self correlation is of size: batch_size x W*H x W x H\n",
    "        # We want the final score to be a score on each pixel and thus of dimension: batch_size x W*H x 1 x 1\n",
    "        # Notation meaning: FG_BG: For WxH map with FG pixels, the channels have non zerovalue where \n",
    "        #                   channel id corresponds to BG != 0\n",
    "        m2_rep = f.adaptive_avg_pool2d(M2s[idy], ker.size()[-2::])\n",
    "\n",
    "        FG_FG = []\n",
    "        FG_BG = []\n",
    "        BG_BG = []\n",
    "        BG_FG = []\n",
    "        for b_iter in range(m2_rep.shape[0]):\n",
    "            FG_FG.append(self.correlate(m2_rep[b_iter].view(-1, 1, 1, 1),m2_rep[b_iter:b_iter+1]))\n",
    "            FG_BG.append(self.correlate(1-m2_rep[b_iter].view(-1, 1, 1, 1),m2_rep[b_iter:b_iter+1]))\n",
    "            BG_FG.append(self.correlate(m2_rep[b_iter].view(-1, 1, 1, 1), 1-m2_rep[b_iter:b_iter+1]))\n",
    "            BG_BG.append(self.correlate(1-m2_rep[b_iter].view(-1, 1, 1, 1),1-m2_rep[b_iter:b_iter+1]))\n",
    "        FG_FG = torch.cat(FG_FG,dim=0)\n",
    "        FG_BG = torch.cat(FG_BG,dim=0)\n",
    "        BG_FG = torch.cat(BG_FG,dim=0)\n",
    "        BG_BG = torch.cat(BG_BG,dim=0)\n",
    "\n",
    "        ############# Loss for adjusting feature extractor\n",
    "        loss_obj_dict = {}\n",
    "\n",
    "        ######### For each FG pixel, find closest 5 pixels in FG and sum them up. That is the -ve loss for FG-FG\n",
    "        num_pixels= 5\n",
    "        ############################## Just template frame\n",
    "        m2_rep_pos = m2_rep.ge(0.5).float()\n",
    "        m2_rep_neg = m2_rep.le(0).float()\n",
    "        if (self.fp16):\n",
    "            m2_rep_pos = m2_rep_pos.half()\n",
    "            m2_rep_neg = m2_rep_pos.half()\n",
    "        indices = torch.nonzero(m2_rep_pos.reshape(-1))\n",
    "        if (len(indices)>0):\n",
    "\n",
    "            FB_FB_loss = (template_self_corr*m2_rep_pos).reshape(-1,W0*H0,W0*H0)\n",
    "            FB_FB_loss, _ = FB_FB_loss.sort(descending=True, dim=2)\n",
    "            FB_FB_loss = torch.cat([FB_FB_loss[:,index,:num_pixels] for index in indices], dim=1)\n",
    "\n",
    "            FB_BG_loss = (template_self_corr*m2_rep_neg).reshape(-1,W0*H0,W0*H0)\n",
    "            FB_BG_loss, _ = FB_BG_loss.sort(descending=True, dim=2)\n",
    "            FB_BG_loss = torch.cat([FB_BG_loss[:,index,:num_pixels] for index in indices], dim=1)\n",
    "\n",
    "            FB_FB_loss = FB_FB_loss.sum()/(len(indices)*num_pixels)\n",
    "            FB_BG_loss = FB_BG_loss.sum()/(len(indices)*num_pixels)\n",
    "        else:\n",
    "            FB_FB_loss = 0\n",
    "            FB_BG_loss = 0\n",
    "\n",
    "        loss_obj_dict['template_FB_FB_loss'] = -FB_FB_loss\n",
    "        loss_obj_dict['template_FB_BG_loss'] = FB_BG_loss\n",
    "\n",
    "        ############################## Just target frame\n",
    "        m1_rep = f.adaptive_avg_pool2d(M1s[idy], ker.size()[-2::])\n",
    "        m1_rep_pos = m1_rep.ge(0.5).float()\n",
    "        m1_rep_neg = m1_rep.le(0).float()\n",
    "        if (self.fp16):\n",
    "            m1_rep_pos = m1_rep_pos.half()\n",
    "            m1_rep_neg = m1_rep_pos.half()\n",
    "        indices = torch.nonzero(m1_rep_pos.reshape(-1))\n",
    "        if (len(indices)>0):\n",
    "\n",
    "            FB_FB_loss = (target_self_corr*m1_rep_pos).reshape(-1,W*H,W*H)\n",
    "            FB_FB_loss, _ = FB_FB_loss.sort(descending=True, dim=2)\n",
    "            FB_FB_loss = torch.cat([FB_FB_loss[:,index,:num_pixels] for index in indices], dim=1)\n",
    "\n",
    "            FB_BG_loss = (target_self_corr*m1_rep_neg).reshape(-1,W*H,W*H)\n",
    "            FB_BG_loss, _ = FB_BG_loss.sort(descending=True, dim=2)\n",
    "            FB_BG_loss = torch.cat([FB_BG_loss[:,index,:num_pixels] for index in indices], dim=1)\n",
    "\n",
    "            FB_FB_loss = FB_FB_loss.sum()/(len(indices)*num_pixels)\n",
    "            FB_BG_loss = FB_BG_loss.sum()/(len(indices)*num_pixels)\n",
    "        else :\n",
    "            FB_FB_loss = 0\n",
    "            FB_BG_loss = 0\n",
    "\n",
    "        loss_obj_dict['target_FB_FB_loss'] = -FB_FB_loss\n",
    "        loss_obj_dict['target_FB_BG_loss'] = FB_BG_loss\n",
    "\n",
    "        ############################## Between template and target\n",
    "\n",
    "        #### Between FG of template and FG of target\n",
    "        # corr_R is of shape: batch_size x W0*H0 x W x H\n",
    "\n",
    "        # We want FG of both to be close and FG-BG of both to be far\n",
    "        FB_FB_loss = (corr_R*m1_rep_pos).reshape(-1,W0*H0,W*H)\n",
    "        indices1 = torch.nonzero(m1_rep_pos.reshape(-1))\n",
    "        indices2 = torch.nonzero(m2_rep_pos.reshape(-1))\n",
    "        corr_R_transpose = corr_R.view(-1, W0*H0, W*H).transpose(1, 2).view(-1, W*H, W0, H0)\n",
    "\n",
    "        if (len(indices1)>0):\n",
    "            FB_FB_loss1, _ = FB_FB_loss.squeeze().transpose(0,1).sort(descending=True, dim=1)\n",
    "            FB_FB_loss1 = torch.cat([FB_FB_loss1[index,:num_pixels] for index in indices1], dim=0)\n",
    "            FB_FB_loss1 = FB_FB_loss1.sum()/(len(indices1)*num_pixels)\n",
    "\n",
    "            FB_BG_loss1 = (corr_R_transpose*m1_rep_neg).reshape(-1,W*H,W0*H0)\n",
    "            FB_BG_loss1, _ = FB_BG_loss1.squeeze().sort(descending=True, dim=1)\n",
    "            FB_BG_loss1 = torch.cat([FB_BG_loss1[index,:num_pixels] for index in indices1], dim=0)\n",
    "            FB_BG_loss1 = FB_BG_loss1.sum()/(len(indices1)*num_pixels)\n",
    "        else:\n",
    "            FB_FB_loss1 = 0\n",
    "            FB_BG_loss1 = 0\n",
    "\n",
    "        if (len(indices2)>0):\n",
    "            FB_FB_loss2, _ = FB_FB_loss.squeeze().sort(descending=True, dim=1)\n",
    "            FB_FB_loss2 = torch.cat([FB_FB_loss2[index,:num_pixels] for index in indices2], dim=0)\n",
    "            FB_FB_loss2 = FB_FB_loss2.sum()/(len(indices2)*num_pixels)\n",
    "\n",
    "            FB_BG_loss2 = (corr_R*m2_rep_neg).reshape(-1,W0*H0,W*H)\n",
    "            FB_BG_loss2, _ = FB_BG_loss2.squeeze().sort(descending=True, dim=1)\n",
    "            FB_BG_loss2 = torch.cat([FB_BG_loss2[index,:num_pixels] for index in indices2], dim=0)\n",
    "            FB_BG_loss2 = FB_BG_loss2.sum()/(len(indices2)*num_pixels)\n",
    "        else:\n",
    "            FB_FB_loss2 = 0\n",
    "            FB_BG_loss2 = 0\n",
    "\n",
    "        loss_obj_dict['tt_FB_FB_loss'] = -(FB_FB_loss1 + FB_FB_loss2)/2\n",
    "        loss_obj_dict['tt_FB_BG_loss'] = (FB_BG_loss1 + FB_BG_loss2)/2\n",
    "        loss_per_obj.append(loss_obj_dict)\n",
    "\n",
    "        # Score addition for machting to FG a lot and penalty for matching to BG\n",
    "        # We are computing FG discriminant score\n",
    "        # For each pixel in WxH, we can take max or average across non zero channels\n",
    "        scale_factor = 0.75\n",
    "        FG_disc_score = f.max_pool2d(FG_FG * template_self_corr, template_self_corr.size()[2::]).view(-1, 1, W0*H0)\n",
    "#                         (f.avg_pool2d(FG_FG , self_corr_R.size()[2::]).view(-1, 1, W0*H0) + 0.000001)\n",
    "        FG_disc_score *= f.relu(\\\n",
    "                -torch.log( (0.000001+f.avg_pool2d(BG_FG * template_self_corr, template_self_corr.size()[2::]).view(-1, 1, W0*H0))/\\\n",
    "                (f.avg_pool2d(BG_FG , template_self_corr.size()[2::]).view(-1, 1, W0*H0) + 0.000001)\\\n",
    "                          )\\\n",
    "                             )\n",
    "\n",
    "        max_only = max_only + FG_disc_score*scale_factor/FG_disc_score.max()\n",
    "        #########  Addition by VJ done #################\n",
    "\n",
    "        # Rank & select FG\n",
    "        m_sorted, m_sorted_idx = max_only.sort(descending=True, dim=2)\n",
    "        corr = torch.cat([co.index_select(0, m_sort[0, 0:256]).unsqueeze(0) for co, m_sort in zip(corr, m_sorted_idx)])\n",
    "        # Merge net FG\n",
    "        corr_fores = self.p_2(self.res_1(self.p_1(f.interpolate(corr, scale_factor=2, mode='bilinear'))))\n",
    "        if max_obj == 1:  # only bg\n",
    "            print('missing obj')\n",
    "            corr_backs = torch.zeros(corr_fores.size()).cuda()\n",
    "        else:\n",
    "            '''\n",
    "            backs_idx = Corr_subs[0:idy] + Corr_subs[idy + 1::]\n",
    "            corr_b = torch.cat(backs_idx, 1)\n",
    "            R_map_b = Rmaps[0:idy] + Rmaps[idy + 1::]\n",
    "            R_map_b = torch.cat(R_map_b, 2)\n",
    "            '''\n",
    "            m2_rep = f.adaptive_avg_pool2d(M2s[idy], ker.size()[-2::])\n",
    "            corr_b = (1-m2_rep.view(m2_rep.size()[0], -1, 1, 1) )* corr_R\n",
    "            R_map_b = (R_map * (1-m2_rep)).view(-1, 1, W0*H0)\n",
    "            ########## Above added by VJ ###########\n",
    "\n",
    "            max_only_b, indices = f.max_pool2d(corr_b, co_size, return_indices=True)\n",
    "            max_only_b = max_only_b.view(R_map_b.size()[0], 1, -1) + R_map_b\n",
    "\n",
    "            ########################################### VJ\n",
    "            BG_disc_score = f.max_pool2d(BG_BG * template_self_corr, template_self_corr.size()[2::]).view(-1, 1, W0*H0)\n",
    "#                         (f.avg_pool2d(FG_FG , self_corr_R.size()[2::]).view(-1, 1, W0*H0) + 0.000001)\n",
    "            BG_disc_score *= f.relu(\\\n",
    "                -torch.log( (0.000001+ f.avg_pool2d(FG_BG * template_self_corr, template_self_corr.size()[2::]).view(-1, 1, W0*H0))/\\\n",
    "                        (f.avg_pool2d(FG_BG , template_self_corr.size()[2::]).view(-1, 1, W0*H0) + 0.000001)\\\n",
    "                          )\\\n",
    "                                   )\n",
    "\n",
    "            max_only_b = max_only_b + BG_disc_score*scale_factor/BG_disc_score.max()\n",
    "            ############################################ VJ\n",
    "\n",
    "            # Rank & select BG\n",
    "            m_sorted, m_sorted_idx = max_only_b.sort(descending=True, dim=2)\n",
    "            corr_b = torch.cat([co.index_select(0, m_sort[0, 0:256]).unsqueeze(0) for co, m_sort in zip(corr_b, m_sorted_idx)])\n",
    "            # Merge net BG\n",
    "            corr_backs = self.p_2(self.res_1(self.p_1(f.interpolate(corr_b, scale_factor=2, mode='bilinear'))))\n",
    "        if idy == 0:\n",
    "            tmp = corr_fores\n",
    "            corr_fores = corr_backs\n",
    "            corr_backs = tmp\n",
    "            m_2 = f.adaptive_avg_pool2d(M2_all, corr_fores.size()[-2::])\n",
    "        else:\n",
    "            m_2 = f.adaptive_avg_pool2d(M2s[idy], corr_fores.size()[-2::])\n",
    "        # low level features\n",
    "        base1 = torch.cat([basef1[idx: idx + 1], corr_fores, corr_backs, m_2], 1)\n",
    "        fea1 = self.R1(base1)\n",
    "        base2 = torch.cat([basef2[idx: idx + 1],\n",
    "                           fea1], 1)\n",
    "        fea2 = self.R2(base2)\n",
    "        base3 = torch.cat([basef3[idx: idx + 1],\n",
    "                           fea2], 1)\n",
    "        fea3 = self.R3(base3)\n",
    "        out = torch.sigmoid(fea3)\n",
    "        Outs.append(out)\n",
    "    Out = torch.cat(Outs, 1)\n",
    "    ############### Once we have out map for all objects in an image, append it to a per image list\n",
    "    Out_Rs.append(Out)\n",
    "    loss_per_batch.append(loss_per_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1737.31689453125"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.cuda as cuda\n",
    "cuda.memory_allocated(0) /(1024*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f79cb62ecf8>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x720 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "idx = 3\n",
    "obj = 1\n",
    "\n",
    "max_obj = template_msk[idx,0].max().int().data.cpu().numpy()\n",
    "target_msk_img = self.P2masks(f.relu(target_msk[idx,0] - 1), max_obj - 1)[obj]\n",
    "template_msk_img = self.P2masks(f.relu(template_msk[idx,0] - 1), max_obj - 1)[obj]\n",
    "plt.subplot(1,4,1)\n",
    "plt.imshow(Out_Rs[idx][0,obj-1].detach().cpu().numpy())\n",
    "plt.subplot(1,4,2)\n",
    "plt.imshow(target_msk_img.detach().cpu().numpy())\n",
    "plt.subplot(1,4,3)\n",
    "plt.imshow(template_msk_img.detach().cpu().numpy())\n",
    "plt.subplot(1,4,4)\n",
    "plt.imshow(template_msk[idx,0].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(870.9578, device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(target_msk[idx,0]-Out_Rs[idx][0,0]).norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(msks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
