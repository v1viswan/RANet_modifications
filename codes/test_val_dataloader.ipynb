{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a RANET val tester function which returns loss on a validation dataset. It uses first frame, first frame mask and prev predicted frame as input and finds the loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from math import log10\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.cuda as cuda\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from RANet_lib import *\n",
    "from RANet_lib.RANet_lib import *\n",
    "from RANet_model import RANet as Net\n",
    "import os\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import PIL.Image as Image\n",
    "\n",
    "from vj_davis_17_loader import Custom_DAVIS2017_dataset, Custom_DAVIS2017_testing_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from vj_loss_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading files from:  ../datasets/DAVIS/ImageSets/2017/train.txt\n"
     ]
    }
   ],
   "source": [
    "from configs_vj.vj_config_ranet_basic import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dtype(data):\n",
    "    return torch._C._TensorBase.float(data)\n",
    "def P2masks(self, P, num):\n",
    "    M = []\n",
    "    M.append(self.Dtype((P == 0) + (P > int(num))))\n",
    "    for idx in range(1, num + 1):\n",
    "        M.append(self.Dtype(P == idx))\n",
    "    return M\n",
    "def get_single_masks(template_msk, target_msk):\n",
    "    for idx in range(len(template_msk)):\n",
    "        max_obj = template_msk[idx,0].max().int().data.cpu().numpy()\n",
    "        target_msk_images = P2masks(F.relu(target_msk[idx,0] - 1), max_obj - 1)\n",
    "        for i in range(max_obj-1):\n",
    "            prediction_single_masks.append(Out[idx][0,i].reshape(-1))\n",
    "            target_single_masks.append(target_msk_images[i+1].reshape(-1))\n",
    "    return \n",
    "def msks2P(msk, objs_ids = None, threshold=0.5):\n",
    "    '''\n",
    "    Msks is expected to be of dimension num_objects x W x H\n",
    "    '''\n",
    "    \n",
    "    if objs_ids is not None and len(msk) != len(objs_ids):\n",
    "        print('error, len(msks) != len(objs_ids)')\n",
    "    return msk.max(dim=0)[0].ge(threshold).long()*(msk.argmax(dim=0)+1)\n",
    "def msk2bbox(msk, k=1.5, inSize1=864, inSize2=480):\n",
    "    '''\n",
    "    msk should be 1xWxH\n",
    "    '''\n",
    "    input_size = [480.0, 864.0]\n",
    "    input_size = [inSize1, inSize2]\n",
    "    if torch.max(msk) == 0:\n",
    "        return torch.from_numpy(np.asarray([0, 0, inSize1, inSize2]))\n",
    "    p = float(input_size[0]) / input_size[1]\n",
    "    msk_x = torch.max(msk[0], 1)[0]\n",
    "    msk_y = torch.max(msk[0], 0)[0]\n",
    "    nzx = torch.nonzero(msk_x)\n",
    "    nzy = torch.nonzero(msk_y)\n",
    "    ## Find coordinates with pixels\n",
    "    bbox_init = [(nzx[0] + nzx[-1]) / 2, (nzy[0] + nzy[-1]) / 2, (nzx[-1] - nzx[0]).float() * k / 2, (nzy[-1] - nzy[0]).float() * k / 2]\n",
    "    # The above selects a box which covers all non zero pixels, with box size = min_box_size * scale factor k * 0.5\n",
    "    # This is like the window size on both directions\n",
    "    # bbox_init coord is like: [ mid_x, mid_y, x_one_side_length, y_one_side_length]\n",
    "    tmp = torch.max(bbox_init[2], p * bbox_init[3])\n",
    "    bbox_init = [bbox_init[0], bbox_init[1], tmp.long(), (tmp / p).long()]\n",
    "    # The above adjusts box shape aspect ratio to the original aspect ratio, with no non zero pixel skipped\n",
    "    \n",
    "    bbox = torch.cat([bbox_init[0] - bbox_init[2], bbox_init[1] - bbox_init[3], bbox_init[0] + bbox_init[2], bbox_init[1] + bbox_init[3]])\n",
    "    # makes dimention to be: [x_min, y_min, x_max, y_max] and Converts to a tensor\n",
    "    bbox = torch.min(torch.max(bbox, torch.zeros(4).cuda().long()),\n",
    "          torch.from_numpy(np.array([input_size[0], input_size[1], input_size[0], input_size[1]])).cuda().long())\n",
    "    # The above bounds the box to range [0, max_img_dim]\n",
    "    if bbox[2] - bbox[0] < 32 or bbox[3] - bbox[1] < 32:\n",
    "        return torch.from_numpy(np.asarray([0, 0, inSize1, inSize2])).cuda()\n",
    "#         return torch.from_numpy(np.asarray([0, 0, 480, 864])).cuda()\n",
    "    return bbox\n",
    "\n",
    "def bbox_crop(img, bbox):\n",
    "    img = img[:, bbox[0]:bbox[2], bbox[1]: bbox[3]]\n",
    "    return img\n",
    "\n",
    "def bbox_uncrop(img, bbox, size, crop_size, inSize1=864, inSize2=480): # 4D input\n",
    "    img = F.interpolate(img, size=crop_size[2::], mode='bilinear',align_corners=True)\n",
    "#     msk = F.pad(img, (bbox[1], 864 - bbox[3], bbox[0], 480 - bbox[2], ))\n",
    "    msk = F.pad(img, (bbox[1], inSize2 - bbox[3], bbox[0], inSize1 - bbox[2], ))\n",
    "    return msk\n",
    "\n",
    "totensor = transforms.ToTensor()\n",
    "def init_Frame(batchsize):\n",
    "    Key_img = [[] for i in range(batchsize)]\n",
    "    Key_mask = [[] for i in range(batchsize)]\n",
    "    Prev_mask = [[] for i in range(batchsize)]\n",
    "    Target_img = [[] for i in range(batchsize)]\n",
    "    Box = [[] for i in range(batchsize)]\n",
    "    Image_names = [[] for i in range(batchsize)]\n",
    "    Frames_batch = dict(Key_img=Key_img, Key_mask=Key_mask, Box=Box, Prev_mask=Prev_mask,\n",
    "                        Target_img=Target_img, Image_names=Image_names)\n",
    "    return Frames_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchsize = 4\n",
    "# start_time = time.perf_counter()\n",
    "# frame_batch = init_Frame(batchsize)\n",
    "# for iteration, batch in enumerate(img_loader, 1):\n",
    "#     frames, masks, names = batch\n",
    "#     print(\"len:\", len(frames), \"count:\", len(frames[0]))\n",
    "#     frame_batch['Key_img'][iteration%batchsize] = frames[0]\n",
    "#     frame_batch['Key_mask'][iteration%batchsize] = masks[0]\n",
    "#     frame_batch['Target_img'][iteration%batchsize].extend(frames[1:])\n",
    "#     frame_batch['Box'][iteration%batchsize] = msk2bbox(masks[0][0].ge(1.6))\n",
    "#     frame_batch['Prev_mask'][iteration%batchsize] = masks[0]\n",
    "#     frame_batch['Image_names'][iteration%batchsize].extend(names)\n",
    "    \n",
    "#     if (iteration%batchsize==0):\n",
    "#         break\n",
    "        \n",
    "# print(\"Time taken to load images:\", time.perf_counter() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict images from mini batch\n",
    "# max_count = max( [len(targets) for targets in frame_batch['Target_img'] ])\n",
    "# for i in range(max_count):\n",
    "#     template = []\n",
    "#     target= []\n",
    "#     template_msk = []\n",
    "#     target_msk = []\n",
    "#     prev_mask = []\n",
    "#     crop_sizes = []\n",
    "#     for j in range(batch_size):\n",
    "#         if (frame_batch['Key_img'][j] == []):\n",
    "#             break\n",
    "#         box = frame_batch['Target_img'][j]\n",
    "        \n",
    "#         template.append(frame_batch['Key_img'][j])\n",
    "#         target.append(frame_batch['Key_mask'][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 30\n",
      "Dirs: 30\n",
      "saving data in X_test\n"
     ]
    }
   ],
   "source": [
    "# img_val_dataset = Custom_DAVIS2017_testing_dataset(root=root, img_shape=img_shape,\\\n",
    "#                                                    img_mode=img_mode, loader_type='val_vj')\n",
    "# img_loader = DataLoader(dataset=img_val_dataset, num_workers=0,\\\n",
    "#                         batch_size=1, shuffle=False, pin_memory=True)\n",
    "inSize1=480\n",
    "inSize2=864\n",
    "dataset='17val'\n",
    "year='2017'\n",
    "mode='test'\n",
    "DAVIS = dict(reading_type='SVOS',\n",
    "             year=year,\n",
    "         root='../datasets/DAVIS/',\n",
    "         subfolder=['', '', ''],\n",
    "         mode=dataset,\n",
    "         tar_mode='rep',\n",
    "         train=0, val=0, test=0, predict=1,\n",
    "         length=None,\n",
    "         init_folder=None,\n",
    "         )\n",
    "dataset = DAVIS2017_loader(\n",
    "    [DAVIS], mode=mode,\n",
    "    transform=[PAD_transform([inSize1, inSize2], random=False),\n",
    "               PAD_transform([inSize1, inSize2], random=False)],\n",
    "    rand=Rand_num())\n",
    "dataset.iter_mode = 'test'\n",
    "data_loader = DataLoader(dataset=dataset, num_workers=0, batch_size=1, shuffle=False, pin_memory=True)\n",
    "\n",
    "toPIL=transforms.ToPILImage()\n",
    "toPIL=transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(ip, t):\n",
    "    # expects batch_size x Wx H binary value images\n",
    "    batch_size = len(ip)\n",
    "    ip = ip.view(batch_size,-1)\n",
    "    t = t.view(batch_size, -1)\n",
    "    intersection = (ip*t).sum(dim=1)\n",
    "    union = ip.sum(dim=1) + t.sum(dim=1) - intersection + 0.0000001 # To avoid zero\n",
    "    return intersection/union\n",
    "\n",
    "def iou_score(predictions, targets, min_objs = 1, names=None):\n",
    "    # Both should be lists with dimensions in batch_size x count_images x W x H\n",
    "    \n",
    "    loss = torch.zeros(1).to(targets[0].device)\n",
    "    for batch, (pred, tar) in enumerate(zip(predictions, targets),1):\n",
    "        if (pred.max() != tar.max()):\n",
    "            print(\"mis matched number of objects! num in pred: {}. num in target: {}\"\\\n",
    "                  .format(pred.max().item(), tar.max().item()) )\n",
    "        \n",
    "        objs_ids = list(set(np.asarray(pred[0].cpu()).reshape(-1)))\n",
    "        max_objs = len(objs_ids)\n",
    "        img_loss = torch.zeros(1).to(tar[0].device)\n",
    "        \n",
    "        for i in range(min_objs, max_objs):\n",
    "            iou = get_iou(pred.eq(i).float(), tar.eq(i).float()).mean()\n",
    "            if (names is not None):\n",
    "                print(\"object:\", names[batch-1] + '_' + str(i), \"iou:\", iou.item())\n",
    "            img_loss += iou\n",
    "        img_loss = img_loss/(max_objs-min_objs)\n",
    "\n",
    "        loss += img_loss\n",
    "    return loss\n",
    "\n",
    "def iou_score(predictions, targets, min_objs = 1, names=None):\n",
    "    # Both should be lists with dimensions in batch_size x count_images x W x H\n",
    "    \n",
    "    loss = torch.zeros(1).to(targets[0].device)\n",
    "    for batch, (pred, tar) in enumerate(zip(predictions, targets),1):\n",
    "        if (pred.max() != tar.max()):\n",
    "            print(\"mis matched number of objects! num in pred: {}. num in target: {}\"\\\n",
    "                  .format(list(set(np.asarray(pred[0].cpu()).reshape(-1))),\\\n",
    "                          list(set(np.asarray(tar[0].cpu()).reshape(-1))) ) )\n",
    "            \n",
    "        objs_ids = list(set(np.asarray(pred[0].cpu()).reshape(-1)))\n",
    "#         print(\"batch:\", batch, \"object ids:\", objs_ids)\n",
    "        max_objs = len(objs_ids)\n",
    "        img_loss = torch.zeros(1).to(tar[0].device)\n",
    "        for i in range(min_objs, max_objs):\n",
    "            iou = get_iou(pred.eq(objs_ids[i]).float(), tar.eq(objs_ids[i]).float()).mean()\n",
    "            if (names is not None):\n",
    "                print(\"object:\", names[batch-1] + '_' + str(i+1-min_objs), \"iou:\", iou.item())\n",
    "            img_loss += iou\n",
    "        if (max_objs-min_objs > 0):\n",
    "            img_loss = img_loss/(max_objs-min_objs)\n",
    "        else:\n",
    "            print(\"No object!!!\", max_objs, min_objs)\n",
    "\n",
    "        loss += img_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_val_loss(data_loader, model, single_object=False, pre_first_frame=False,\\\n",
    "                          batchsize=4, disc_scale=0):\n",
    "#     ms = [864, 480]\n",
    "    palette_path = '../datasets/palette.txt'\n",
    "    with open(palette_path) as f:\n",
    "        palette = f.readlines()\n",
    "    palette = list(np.asarray([[int(p) for p in pal[0:-1].split(' ')] for pal in palette]).reshape(768))\n",
    "    def init_Frame(batchsize):\n",
    "        Key_features = [[] for i in range(batchsize)]\n",
    "        Key_masks = [[] for i in range(batchsize)]\n",
    "        Init_Key_masks = [[] for i in range(batchsize)]\n",
    "        Frames = [[] for i in range(batchsize)]\n",
    "        Box = [[] for i in range(batchsize)]\n",
    "        Image_names = [[] for i in range(batchsize)]\n",
    "        Img_sizes = [[] for i in range(batchsize)]\n",
    "        Frames_batch = dict(Frames=Frames, Key_features=Key_features, Key_masks=Key_masks,\\\n",
    "                            Box=Box, Img_sizes=Img_sizes, Init_Key_masks=Init_Key_masks,\\\n",
    "                            Image_names=Image_names, Sizes=[0 for i in range(batchsize + 1)],\\\n",
    "                            batchsize=batchsize, Flags=[[] for i in range(batchsize)],\\\n",
    "                            Img_flags=[[] for i in range(batchsize)], Target_msk=[[] for i in range(batchsize)])\n",
    "        return Frames_batch\n",
    "    max_iter = batchsize\n",
    "    torch.set_grad_enabled(False)\n",
    "    _ = None\n",
    "    Frames_batch = init_Frame(batchsize)\n",
    "    \n",
    "    score = 0\n",
    "    print('Loading validation Data ........., len:',len(data_loader))\n",
    "    name_list = []\n",
    "    for iteration, batch in enumerate(data_loader, 1):\n",
    "\n",
    "        batch[0] = [datas.cuda() for datas in batch[0]]\n",
    "        batch[1] = [datas.cuda() for datas in batch[1]]\n",
    "        \n",
    "        frame_num = len(batch[0])\n",
    "        Key_frame = batch[0][0]\n",
    "        init_Key_mask = batch[1][0]\n",
    "        size = Key_frame.size()[2::]\n",
    "        # cc for key frame\n",
    "        bbox = msk2bbox(init_Key_mask[0].ge(1.6), k=1.5, inSize1=inSize1, inSize2=inSize2)\n",
    "        Key_frame = F.interpolate(bbox_crop(Key_frame[0], bbox).unsqueeze(0), size,\\\n",
    "                                  mode='bilinear',align_corners=True)\n",
    "        Key_mask = F.interpolate(bbox_crop(init_Key_mask[0], bbox).unsqueeze(0), size)\n",
    "        S_name = batch[2][0][0]\n",
    "        Key_feature = model(_, Key_frame, _, _, mode='first', disc_scale=disc_scale)[0]\n",
    "        Frames = batch[0]\n",
    "        Img_sizes = batch[3]\n",
    "\n",
    "        loc = np.argmin(Frames_batch['Sizes'][0:batchsize])\n",
    "\n",
    "        Fsize = len(batch[2])\n",
    "        # print(loc)\n",
    "#         print(\"folder:\", S_name, \"images:\", len(batch[0]))\n",
    "        \n",
    "        Frames_batch['Frames'][loc].extend(Frames[1::])\n",
    "        Frames_batch['Key_features'][loc].extend([Key_feature] + [None] * (Fsize - 2))\n",
    "        Frames_batch['Key_masks'][loc].extend([Key_mask] * (Fsize - 1))\n",
    "        Frames_batch['Init_Key_masks'][loc].extend([init_Key_mask] * (Fsize - 1))\n",
    "        Frames_batch['Box'][loc].extend([bbox] + [None] * (Fsize - 2))\n",
    "        Frames_batch['Flags'][loc].extend([1] + [2 for i in range(Fsize - 3)] + [3])\n",
    "        Frames_batch['Sizes'][loc] += Fsize - 1\n",
    "\n",
    "        Frames_batch['Image_names'][loc].extend([b[0] for b in batch[2]])\n",
    "        Frames_batch['Img_sizes'][loc].extend(Img_sizes)\n",
    "        Frames_batch['Img_flags'][loc].extend([1] + [2 for i in range(Fsize - 2)] + [3])\n",
    "        Frames_batch['Target_msk'][loc].extend(batch[1][:])\n",
    "        \n",
    "        name_list.append(S_name.split('480p/')[1].split('/')[0])\n",
    "        \n",
    "        if iteration % max_iter == 0 or iteration == len(data_loader):\n",
    "#             print(\"Sending val batch of images for prediction, iteration:\", iteration)\n",
    "            for idx in range(batchsize):\n",
    "                Frames_batch['Flags'][idx].append(False)\n",
    "            Frames_batch['Sizes'][batchsize] = min(Frames_batch['Sizes'][0:batchsize])# - 1])\n",
    "            threshold=0.5\n",
    "            Out_Mask = process_SVOS_batch(Frames_batch, model, threshold, single_object, pre_first_frame,\\\n",
    "                                          disc_scale=disc_scale)\n",
    "            target_mask = Frames_batch['Target_msk']\n",
    "            outs = [torch.from_numpy(np.stack(out)).to(target_mask[0][0].device) for out in Out_Mask]\n",
    "            tmasks = [torch.cat(mask).squeeze() for mask in target_mask if mask != []]\n",
    "            for i in range(len(outs)):\n",
    "                if np.isnan(outs[i].sum().item()):\n",
    "                    print(\"nan value in outs i :\", i)\n",
    "                    asdsad\n",
    "#             return outs, tmasks, name_list\n",
    "\n",
    "            ################## Save masks in a folder ##################\n",
    "#             save_root = '../predictions/IOU_disc_scale_05_epoch0_re/'\n",
    "#             if not os.path.exists(save_root):\n",
    "#                 os.mkdir(save_root)\n",
    "#             for name, out in zip(name_list, outs):\n",
    "#                 op_images = F.interpolate(out.unsqueeze(0),size=(480,910), mode='nearest')[0]\n",
    "#                 op_images = [Image.fromarray(img.astype('float32')-1).convert('P')\\\n",
    "#                                      for img in op_images.detach().cpu().numpy()]\n",
    "\n",
    "#                 if not os.path.exists(save_root+name+'/'):\n",
    "#                     os.mkdir(save_root+name+'/')\n",
    "#                 for i in range(len(op_images)):\n",
    "#                     op_images[i].putpalette(palette)\n",
    "#                     op_images[i].save(save_root+name+'/'+str(i).zfill(5)+'.png')\n",
    "            ################## Done saving predicted masks ##################\n",
    "            \n",
    "            score += iou_score(outs, tmasks,  names=name_list, min_objs=2).item()\n",
    "            print(\"Iteration: {}, val iou_score so far: {}, avg: {}\"\\\n",
    "                  .format(iteration, score, score/iteration))\n",
    "            Frames_batch = init_Frame(batchsize)\n",
    "            name_list = []\n",
    "            del outs, tmasks, Out_Mask, target_mask\n",
    "    return score/iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-object mode\n",
      "=> Loaded checkpoint '../models/RANet_video_multi_IOU_disc_scale_05_epoch0.pth'\n",
      "model ready\n"
     ]
    }
   ],
   "source": [
    "model = Net(pretrained=False, type='multi_object')\n",
    "# inSize1 = 480\n",
    "# inSize2 = 864\n",
    "\n",
    "model.set_type('multi_object')\n",
    "#RANet_video_multi_IOU_trnsfm_epoch2.pth\n",
    "#RANet_video_multi_IOU_disc_scale_05_epoch0.pth\n",
    "#RANet_video_multi\n",
    "#RANet_video_multi_BCE_epoch3.pth\n",
    "#RANet_video_multi_IOU_trnsfm_nms_epoch2.pth\n",
    "checkpoint_load('../models/' + 'RANet_video_multi_IOU_disc_scale_05_epoch0.pth', model)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "print(\"model ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score  with RANet_video_multi <br>\n",
    "score  with RANet_video_multi_IOU_trnsfm_epoch2 <br>\n",
    "score  with RANet_video_multi_BCE_epoch3 <br>\n",
    "score 0.7533618927001953 with RANet_video_multi_IOU_disc_scale_05_epoch0 <br>\n",
    "score  with RANet_video_multi_IOU_trnsfm_nms_epoch2 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading validation Data ........., len: 30\n",
      "object: bike-packing_1 iou: 0.6084873676300049\n",
      "object: bike-packing_2 iou: 0.7943460941314697\n",
      "object: blackswan_1 iou: 0.9519302248954773\n",
      "object: bmx-trees_1 iou: 0.5122173428535461\n",
      "object: bmx-trees_2 iou: 0.7441556453704834\n",
      "object: breakdance_1 iou: 0.7587180733680725\n",
      "Iteration: 4, val iou_score so far: 3.0402514934539795, avg: 0.7600628733634949\n",
      "object: camel_1 iou: 0.9433619976043701\n",
      "object: car-roundabout_1 iou: 0.9743897914886475\n",
      "object: car-shadow_1 iou: 0.9691945314407349\n",
      "object: cows_1 iou: 0.9501351714134216\n",
      "Iteration: 8, val iou_score so far: 6.877332925796509, avg: 0.8596666157245636\n",
      "object: dance-twirl_1 iou: 0.8747526407241821\n",
      "object: dog_1 iou: 0.9543012976646423\n",
      "object: dogs-jump_1 iou: 0.1226084902882576\n",
      "object: dogs-jump_2 iou: 0.5742332339286804\n",
      "object: dogs-jump_3 iou: 0.9280974864959717\n",
      "object: drift-chicane_1 iou: 0.6979482173919678\n",
      "Iteration: 12, val iou_score so far: 9.945981502532959, avg: 0.8288317918777466\n",
      "object: drift-straight_1 iou: 0.8999441266059875\n",
      "object: goat_1 iou: 0.8857313394546509\n",
      "object: gold-fish_1 iou: 0.7504079341888428\n",
      "object: gold-fish_2 iou: 0.7158799171447754\n",
      "object: gold-fish_3 iou: 0.7432247400283813\n",
      "object: gold-fish_4 iou: 0.8740588426589966\n",
      "object: gold-fish_5 iou: 0.8543870449066162\n",
      "object: horsejump-high_1 iou: 0.8690069913864136\n",
      "object: horsejump-high_2 iou: 0.8105096817016602\n",
      "Iteration: 16, val iou_score so far: 13.359007120132446, avg: 0.8349379450082779\n",
      "object: india_1 iou: 0.6128411293029785\n",
      "object: india_2 iou: 0.48879608511924744\n",
      "object: india_3 iou: 0.45653802156448364\n",
      "object: judo_1 iou: 0.8063279986381531\n",
      "object: judo_2 iou: 0.702164888381958\n",
      "object: kite-surf_1 iou: 0.46086299419403076\n",
      "object: kite-surf_2 iou: 0.24264292418956757\n",
      "object: kite-surf_3 iou: 0.5068931579589844\n",
      "object: lab-coat_1 iou: 0.021276595070958138\n",
      "object: lab-coat_2 iou: 0.021276595070958138\n",
      "object: lab-coat_3 iou: 0.8785597085952759\n",
      "object: lab-coat_4 iou: 0.8158363103866577\n",
      "object: lab-coat_5 iou: 0.9134570956230164\n",
      "Iteration: 20, val iou_score so far: 15.566193103790283, avg: 0.7783096551895141\n",
      "object: libby_1 iou: 0.8867892026901245\n",
      "object: loading_1 iou: 0.9565284252166748\n",
      "object: loading_2 iou: 0.22591620683670044\n",
      "object: loading_3 iou: 0.840783953666687\n",
      "object: mbike-trick_1 iou: 0.7967157363891602\n",
      "object: mbike-trick_2 iou: 0.7050701379776001\n",
      "object: motocross-jump_1 iou: 0.462315171957016\n",
      "object: motocross-jump_2 iou: 0.5001521110534668\n",
      "Iteration: 24, val iou_score so far: 18.35951828956604, avg: 0.7649799287319183\n",
      "object: paragliding-launch_1 iou: 0.7302285432815552\n",
      "object: paragliding-launch_2 iou: 0.5640221834182739\n",
      "object: paragliding-launch_3 iou: 0.13256330788135529\n",
      "object: parkour_1 iou: 0.9412171840667725\n",
      "object: pigs_1 iou: 0.7763585448265076\n",
      "object: pigs_2 iou: 0.5652985572814941\n",
      "object: pigs_3 iou: 0.9472744464874268\n",
      "object: scooter-black_1 iou: 0.5718997120857239\n",
      "object: scooter-black_2 iou: 0.8218781352043152\n",
      "Iteration: 28, val iou_score so far: 21.23620629310608, avg: 0.7584359390395028\n",
      "object: shooting_1 iou: 0.2434920072555542\n",
      "object: shooting_2 iou: 0.7906503081321716\n",
      "object: shooting_3 iou: 0.7294769883155823\n",
      "object: soapbox_1 iou: 0.8742823600769043\n",
      "object: soapbox_2 iou: 0.7828404307365417\n",
      "object: soapbox_3 iou: 0.6732091307640076\n",
      "Iteration: 30, val iou_score so far: 22.60085678100586, avg: 0.7533618927001953\n",
      "Took time: 804.1258978899568  IOU score: 0.7533618927001953\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "with torch.no_grad():\n",
    "    score = get_val_loss(data_loader, model, batchsize=4, disc_scale=0.5)\n",
    "print(\"Took time:\", time.perf_counter() - start, \" IOU score:\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     outs, tmasks, name_list = test_SVOS_Video_batch(data_loader, model, cirterion=None, batchsize=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# palette_path = '../datasets/palette.txt'\n",
    "# with open(palette_path) as f:\n",
    "#     palette = f.readlines()\n",
    "# palette = list(np.asarray([[int(p) for p in pal[0:-1].split(' ')] for pal in palette]).reshape(768))\n",
    "\n",
    "# save_root = '../predictions/IOU_disc_scale_05_epoch0_re/'\n",
    "# if not os.path.exists(save_root):\n",
    "#     os.mkdir(save_root)\n",
    "# for name, out in zip(name_list, outs):\n",
    "#     op_images = F.interpolate(out.unsqueeze(0),size=(480,910), mode='nearest')[0]\n",
    "#     op_images = out\n",
    "#     op_images = [Image.fromarray(img.astype('float32')-1).convert('P')\\\n",
    "#                                                                   for img in op_images.detach().cpu().numpy()]\n",
    "\n",
    "#     if not os.path.exists(save_root+name+'/'):\n",
    "#         os.mkdir(save_root+name+'/')\n",
    "#     for i in range(len(op_images)):\n",
    "#         op_images[i].putpalette(palette)\n",
    "#         op_images[i].save(save_root+name+'/'+str(i).zfill(5)+'.png')\n",
    "# iou_score(outs, tmasks,  names=name_list, min_objs=2).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
