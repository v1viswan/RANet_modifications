{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files: 30\n",
      "Dirs: 30\n",
      "saving data in X_test\n",
      "loading files from:  ../datasets/DAVIS/ImageSets/2017/train.txt\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "from RANet_lib import *\n",
    "from RANet_lib.RANet_lib import *\n",
    "from RANet_model import RANet as Net\n",
    "from RANet_model import make_layer2, MS_Block, ResBlock2\n",
    "import os\n",
    "import os.path as osp\n",
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "import PIL.Image as Image\n",
    "\n",
    "from vj_davis_17_loader import Custom_DAVIS2017_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from vj_loss_functions import *\n",
    "from vj_data_parallel_model import *\n",
    "\n",
    "import argparse\n",
    "from math import log10\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.cuda as cuda\n",
    "import torch.multiprocessing\n",
    "\n",
    "from configs_vj.vj_config_ranet_iou_trnsfm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class support_model(nn.Module):\n",
    "    def __init__(self, ip_channel, level = 0):\n",
    "        super(support_model, self).__init__()\n",
    "        ch1 = min(64, ip_channel)\n",
    "        self.predictor = nn.Sequential(make_layer2(ip_channel, ch1),\n",
    "                make_layer2(ch1, 32),\n",
    "                MS_Block(32, 16, d=[1,3,6]),\n",
    "                ResBlock2(16, 8),\n",
    "                nn.Conv2d(16, 1, 3, padding=1)\n",
    "                )\n",
    "        self.level = level\n",
    "    def forward(self, feat):\n",
    "        Out = []\n",
    "        for idx in range(len(feat)): # Per image\n",
    "            out_ = []\n",
    "            for idy in range(len(feat[idx])): # Per object in image\n",
    "                # RANet upsamples just after features are computer. So down sample it back\n",
    "                feat_img = F.interpolate(feat[idx][idy][self.level],scale_factor=1/2,\\\n",
    "                                         mode='bilinear', align_corners=True)\n",
    "                out_.append(self.predictor(feat_img))\n",
    "            Out.append(torch.cat(out_, 1))\n",
    "        return Out\n",
    "class Full_training_RAnet_multi_level(nn.Module):\n",
    "    '''\n",
    "    Takes Loss function for segmentation, the segmentation model and \n",
    "    '''\n",
    "    def __init__(self, model, loss_classifier, sup_model1, sup_model2, lamda1=0.5, lamda2=0.5):\n",
    "        super(Full_training_RAnet_multi_level, self).__init__()\n",
    "        self.model = model\n",
    "        self.loss_classifier = loss_classifier\n",
    "        self.sup_model1 = sup_model1\n",
    "        self.sup_model2 = sup_model2\n",
    "        self.lamda1 = lamda1 # How much to weigh the 1st level mask prediction loss\n",
    "        self.lamda2 = lamda2 # How much to weigh the 2nd level mask prediction loss\n",
    "        \n",
    "    def forward(self, template, target, template_msk, target_msk,\\\n",
    "                               prev_mask=None):\n",
    "\n",
    "        Out, feat = self.model.RANet_Multiple_forward_train_mult_lvl(template=template,target=target,\\\n",
    "                               template_msk=template_msk, target_msk = target_msk,prev_mask=prev_mask)\n",
    "        ############ Main mask prediction #############\n",
    "        prediction_single_masks = []\n",
    "        target_single_masks = []\n",
    "        for idx in range(len(Out)):\n",
    "            max_obj = template_msk[idx,0].max().int().data.cpu().numpy()\n",
    "            target_msk_images = self.model.P2masks(F.relu(target_msk[idx,0] - 1), max_obj - 1)\n",
    "            for i in range(max_obj-1):\n",
    "                prediction_single_masks.append(Out[idx][0,i].reshape(-1))\n",
    "                target_single_masks.append(target_msk_images[i+1].reshape(-1))\n",
    "\n",
    "        prediction_single_masks = torch.stack(prediction_single_masks)\n",
    "        target_single_masks = torch.stack(target_single_masks)\n",
    "        \n",
    "        cls_loss = self.loss_classifier(prediction_single_masks,target_single_masks)\n",
    "        cls_loss_lvl1 = cls_loss.clone()*0\n",
    "        cls_loss_lvl2 = 0\n",
    "        \n",
    "        Out_lvl1 = self.sup_model1(feat)\n",
    "        Out_lvl2 = self.sup_model2(feat)\n",
    "\n",
    "        ########## Level 1 prediction ###############3\n",
    "        w,h = Out_lvl1[0][0][0].shape[-2:]\n",
    "        prediction_single_masks = []\n",
    "        target_single_masks = []\n",
    "        target_msk_lvl1 = F.interpolate(target_msk, size=(w,h), mode='nearest')\n",
    "\n",
    "        for idx in range(len(Out)): # Number of images\n",
    "            max_obj = template_msk[idx,0].max().int().data.cpu().numpy()\n",
    "            target_msk_images = self.model.P2masks(F.relu( target_msk_lvl1[idx,0] - 1), max_obj - 1)\n",
    "            for i in range(max_obj-1):\n",
    "                prediction_single_masks.append(Out_lvl1[idx][0,i].reshape(-1))\n",
    "                target_single_masks.append(target_msk_images[i+1].reshape(-1))\n",
    "\n",
    "        prediction_single_masks = torch.stack(prediction_single_masks)\n",
    "        target_single_masks = torch.stack(target_single_masks)\n",
    "\n",
    "        cls_loss_lvl1 = loss_fn(prediction_single_masks,target_single_masks)\n",
    "\n",
    "        ########## Level 2 prediction ###############3\n",
    "        w,h = Out_lvl2[0][0][0].shape[-2:]\n",
    "        prediction_single_masks = []\n",
    "        target_single_masks = []\n",
    "        target_msk_lvl1 = F.interpolate(target_msk, size=(w,h), mode='nearest')\n",
    "\n",
    "        for idx in range(len(Out)):\n",
    "            max_obj = template_msk[idx,0].max().int().data.cpu().numpy()\n",
    "            target_msk_images = self.model.P2masks(F.relu( target_msk_lvl1[idx,0] - 1), max_obj - 1)\n",
    "            for i in range(max_obj-1):\n",
    "                prediction_single_masks.append(Out_lvl2[idx][0,i].reshape(-1))\n",
    "                target_single_masks.append(target_msk_images[i+1].reshape(-1))\n",
    "\n",
    "        prediction_single_masks = torch.stack(prediction_single_masks)\n",
    "        target_single_masks = torch.stack(target_single_masks)\n",
    "\n",
    "        cls_loss_lvl2 = loss_fn(prediction_single_masks,target_single_masks)\n",
    "        \n",
    "        total_loss = cls_loss + self.lamda1*cls_loss_lvl1 + self.lamda2*cls_loss_lvl2\n",
    "        \n",
    "        return torch.stack((total_loss, cls_loss, cls_loss_lvl1+cls_loss_lvl2)).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using GPUs ID: [0]\n",
      "loading files from:  ../datasets/DAVIS/ImageSets/2017/train.txt\n",
      "===> Building model\n",
      "Multi-object mode\n",
      "=> Loaded checkpoint '../models/RANet_video_multi.pth'\n",
      "Support modules ready\n",
      "memory usage : 246.8291015625\n"
     ]
    }
   ],
   "source": [
    "dataset='17train'\n",
    "inSize1 = 480\n",
    "inSize2 = 864\n",
    "root = '../datasets/DAVIS'\n",
    "img_mode = '480p'\n",
    "img_shape = (inSize1,inSize2)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpus = [i for i in range(torch.cuda.device_count())]\n",
    "print('using GPUs ID: {}'.format(gpus))\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "trnsfm_crop = RandomCrop(min_size=0.75, prob=0.3)\n",
    "trnsfm_rotate=RandomRotate(max_angle=np.pi/6, prob=0.3)\n",
    "trnsfm_flip=RandomFlip(vprob=0.2, hprob=0.2)\n",
    "#transforms.Compose([trnsfm_crop,trnsfm_rotate,trnsfm_flip])\n",
    "\n",
    "trnsfm_piecewise = transforms.Compose([trnsfm_crop,trnsfm_rotate]) \n",
    "trnsfm_common = transforms.Compose([trnsfm_flip])\n",
    "get_prev_mask = True\n",
    "use_std_template = True\n",
    "\n",
    "img_dataset = Custom_DAVIS2017_dataset(root=root, img_shape=img_shape, img_mode=img_mode,\\\n",
    "                            get_prev_mask=get_prev_mask, use_std_template=use_std_template,\\\n",
    "                            trnsfm_common = None, trnsfm_piecewise=None,loader_type='train')\n",
    "\n",
    "img_loader = DataLoader(dataset=img_dataset, num_workers=0,\\\n",
    "                        batch_size=batch_size*len(gpus), shuffle=False, pin_memory=True)\n",
    "\n",
    "print('===> Building model')\n",
    "############## Choose a model #################\n",
    "# params='RANet_video_multi.pth'\n",
    "# params='RANet_encoder_retrain_epoch1.pth'\n",
    "# params='RANet_multi_basic_train_epoch1.pth'\n",
    "params = 'RANet_video_multi.pth'\n",
    "model = Net(pretrained=False, type=net_type)\n",
    "model.set_type(net_type)\n",
    "# model.cuda()\n",
    "checkpoint_load('../models/' + params, model)\n",
    "\n",
    "optimizer_model = torch.optim.Adam(model.parameters(), lr=0)\n",
    "\n",
    "####### Support models #########\n",
    "sup_model1= support_model(ip_channel=128, level = 0)\n",
    "sup_model2= support_model(ip_channel=32, level = 1)\n",
    "\n",
    "try:\n",
    "    sup_model1.load_state_dict(torch.load('../models/sup_model1.pth'))\n",
    "    sup_model2.load_state_dict(torch.load('../models/sup_model2.pth'))\n",
    "except:\n",
    "    print(\"Creating support models from scratch\")\n",
    "\n",
    "optimizer_sup_model1 = torch.optim.Adam(sup_model1.parameters(), lr=1e-04)\n",
    "optimizer_sup_model2 = torch.optim.Adam(sup_model2.parameters(), lr=1e-04)\n",
    "print(\"Support modules ready\")\n",
    "############## Data parallel Model ###############\n",
    "full_model = Full_training_RAnet_multi_level(model, loss_classifier=loss_fn,\\\n",
    "                    sup_model1=sup_model1, sup_model2=sup_model2, lamda1=lamda1, lamda2=lamda2)\n",
    "# full_model = full_model.cuda()\n",
    "\n",
    "# parallel_model_parameters = []\n",
    "# for name, param in full_model.named_parameters():\n",
    "#     if ('model' not in name):\n",
    "#         parallel_model_parameters.append(param)\n",
    "\n",
    "# optimizer_parallel_model = torch.optim.Adam(parallel_model_parameters, lr=0)\n",
    "full_model = nn.DataParallel(full_model, device_ids=gpus).cuda()\n",
    "print(\"memory usage :\", cuda.memory_allocated(0) /(1024*1024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model = False\n",
    "train_sup_model = True\n",
    "epoch = 1\n",
    "max_memory = cuda.memory_allocated(0) /(1024*1024)\n",
    "loss_per_epoch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2423: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New max memory!: 3398.55810546875 iteration: 1\n",
      "New max memory!: 4358.0888671875 iteration: 2\n",
      "New max memory!: 5052.8505859375 iteration: 4\n",
      "New max memory!: 5053.1005859375 iteration: 9\n",
      "New max memory!: 6442.375 iteration: 16\n",
      "New max memory!: 6443.625 iteration: 19\n",
      "New max memory!: 8525.4091796875 iteration: 29\n",
      "epoch: 0 level loss: 0.3402410996456941 Time for mini batch: 76.62341135973111 time spend on model running: 34.61787291103974 memory used 511.59814453125\n",
      "epoch: 1 level loss: 0.3674257849653562 Time for mini batch: 70.62205483717844 time spend on model running: 34.70563895441592 memory used 511.59814453125\n",
      "epoch: 2 level loss: 0.3774564017852147 Time for mini batch: 70.2120343260467 time spend on model running: 34.72170540271327 memory used 511.59814453125\n",
      "epoch: 3 level loss: 0.3975088447332382 Time for mini batch: 70.47939829900861 time spend on model running: 34.734846849925816 memory used 511.59814453125\n",
      "epoch: 4 level loss: 0.3860672796765963 Time for mini batch: 69.66295719565824 time spend on model running: 34.798157162964344 memory used 511.59814453125\n",
      "epoch: 5 level loss: 0.39492231731613475 Time for mini batch: 71.09023758396506 time spend on model running: 34.78250316204503 memory used 511.59814453125\n",
      "epoch: 6 level loss: 0.3921707453827063 Time for mini batch: 71.2673994670622 time spend on model running: 34.77397634508088 memory used 511.59814453125\n",
      "epoch: 7 level loss: 0.3875902353475491 Time for mini batch: 71.35064495913684 time spend on model running: 34.80254392232746 memory used 511.59814453125\n",
      "epoch: 8 level loss: 0.4002879918863376 Time for mini batch: 70.37794844480231 time spend on model running: 34.82532716821879 memory used 511.59814453125\n",
      "epoch: 9 level loss: 0.42141020645697913 Time for mini batch: 70.74415043089539 time spend on model running: 34.713852513581514 memory used 511.59814453125\n",
      "epoch: 10 level loss: 0.4292985419432322 Time for mini batch: 70.85670997481793 time spend on model running: 34.76256900886074 memory used 511.59814453125\n",
      "epoch: 11 level loss: 0.4556185300151507 Time for mini batch: 70.61043393332511 time spend on model running: 34.79501365683973 memory used 511.59814453125\n",
      "epoch: 12 level loss: 0.4528457763294379 Time for mini batch: 70.5010596611537 time spend on model running: 34.82647539768368 memory used 511.59814453125\n",
      "epoch: 13 level loss: 0.41902176986138023 Time for mini batch: 70.85233803419396 time spend on model running: 34.79752198513597 memory used 511.59814453125\n",
      "epoch: 14 level loss: 0.409516837199529 Time for mini batch: 71.06269652489573 time spend on model running: 34.74668413726613 memory used 511.59814453125\n",
      "epoch: 15 level loss: 0.40387385139862697 Time for mini batch: 71.42681129369885 time spend on model running: 34.8550227037631 memory used 511.59814453125\n",
      "epoch: 16 level loss: 0.4094218644003073 Time for mini batch: 72.50590175017715 time spend on model running: 34.81959840748459 memory used 511.59814453125\n",
      "epoch: 17 level loss: 0.4281578451395035 Time for mini batch: 71.00798144284636 time spend on model running: 34.83405994903296 memory used 511.59814453125\n",
      "epoch: 18 level loss: 0.41165680612126987 Time for mini batch: 70.59208569396287 time spend on model running: 34.85832767048851 memory used 511.59814453125\n",
      "epoch: 19 level loss: 0.3982254939774672 Time for mini batch: 69.85885179182515 time spend on model running: 34.857679151464254 memory used 511.59814453125\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(20):\n",
    "    start_time = time.perf_counter()\n",
    "    loss_per_batch = []\n",
    "    model_train_time = 0\n",
    "    if (train_model):\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    if (train_sup_model):\n",
    "        sup_model1.train()\n",
    "        sup_model2.train()\n",
    "    else:\n",
    "        sup_model1.eval()\n",
    "        sup_model2.eval()\n",
    "    \n",
    "    for iteration, batch in enumerate(img_loader, 1):\n",
    "        template,template_mask, target,target_mask, prev_mask = batch\n",
    "\n",
    "        optimizer_model.zero_grad()\n",
    "        optimizer_sup_model1.zero_grad()\n",
    "        optimizer_sup_model2.zero_grad()\n",
    "\n",
    "        start_time_model = time.perf_counter()\n",
    "        loss = full_model(template=template, target=target, template_msk=template_mask, target_msk=target_mask,\\\n",
    "                              prev_mask=prev_mask)\n",
    "        total_loss, cls_loss, cls_loss_lvls = loss.mean(dim=0)\n",
    "        total_loss= total_loss.mean()\n",
    "\n",
    "        if (cuda.memory_allocated(0) /(1024*1024) > max_memory):\n",
    "            print(\"New max memory!:\", cuda.memory_allocated(0) /(1024*1024), \"iteration:\", iteration)\n",
    "            max_memory = cuda.memory_allocated(0) /(1024*1024)\n",
    "        total_loss.backward()\n",
    "        if train_model and epoch > -1:\n",
    "            optimizer_model.step()\n",
    "        if train_sup_model and epoch > -1:\n",
    "            optimizer_sup_model1.step()\n",
    "            optimizer_sup_model2.step()\n",
    "            \n",
    "        loss_per_batch.append([total_loss.item(), cls_loss.item(), cls_loss_lvls.item()])\n",
    "        del total_loss, cls_loss, cls_loss_lvls, loss, template,template_mask, target,target_mask, prev_mask\n",
    "        model_train_time += time.perf_counter() - start_time_model\n",
    "        \n",
    "    if (train_sup_model):\n",
    "        torch.save(sup_model1.state_dict(), '../models/sup_model1.pth')\n",
    "        torch.save(sup_model2.state_dict(), '../models/sup_model2.pth')\n",
    "    \n",
    "    loss_per_batch = np.array(loss_per_batch)\n",
    "    loss_per_epoch.append(np.mean(loss_per_batch, axis=0))\n",
    "    memory = cuda.memory_allocated(0) /(1024*1024)\n",
    "    end_time = time.perf_counter()\n",
    "    \n",
    "    print(\"epoch:\", epoch, \"level loss:\",loss_per_epoch[-1][2], \"Time for mini batch:\", end_time - start_time,\\\n",
    "      \"time spend on model running:\",model_train_time, \"memory used\",memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8533.2841796875"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
